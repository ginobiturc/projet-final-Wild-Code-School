# -*- coding: utf-8 -*-
"""Projet3-CKD-DF7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10UF95c8JJWgI3QPUK-nrXNK7Xx3w0raV
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
#pour les différents Scaler
from sklearn.preprocessing import MaxAbsScaler, QuantileTransformer, PowerTransformer, MinMaxScaler, StandardScaler, RobustScaler
#pour les différents modèles de ML
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier
from sklearn.svm import LinearSVC, NuSVC, SVC
from sklearn.naive_bayes import GaussianNB
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier

#pour les metrics
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import recall_score

from pycaret.classification import *

# pour voir toutes les colonnes
pd.set_option('display.max_columns', None)

df = pd.read_csv(r'df_ckd_7.csv')

"""# Test PYCARET"""

df.corr().classification

df

clf1 = setup(data=df, target='classification')

best = compare_models()

"""# Choix des variables"""

df

# 13 variables choisies pour X
#X = df[['hemo','packed_cell_vol','red_blood_cell', 'blood_g_random','sod','albumine','sugar','blood_urea', 'hypertension','age','blood_pressure','diabete']]
#y = df['classification']
#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size = 0.75)

# toutes les variables
X = df.select_dtypes(include = 'number').drop(columns= ['classification', 'nan_count', 'id', 'specific_gravity'])
y= df['classification']
#X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, random_state=42, train_size = 0.75)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size = 0.75)

print("The length of the initial dataset is :", len(X))
print("The length of the train dataset is   :", len(X_train))
print("The length of the test dataset is    :", len(X_test))

"""# Test sans scaler"""

modelKNN = KNeighborsClassifier()

modelKNN.fit(X_train,y_train)

print(' accuracy score Train: ', modelKNN.score(X_train, y_train))
print(' accuracy score Test: ', modelKNN.score(X_test, y_test))

modelKNN.fit(X2_train,y2_train)

print(' accuracy score Train: ', modelKNN.score(X2_train, y2_train))
print(' accuracy score Test: ', modelKNN.score(X2_test, y2_test))

# Le score est beaucoup plus intéressant avec les 13 variables choisies

"""# Machine learning"""

list_scaler = [MaxAbsScaler(), QuantileTransformer(), PowerTransformer(),
               MinMaxScaler(), StandardScaler(), RobustScaler()]

##liste des modèles utilisés
list_model = [
    KNeighborsClassifier(),
    DecisionTreeClassifier(),
    LogisticRegression(),
    ExtraTreesClassifier(),
    RandomForestClassifier(),
    GradientBoostingClassifier(),
    AdaBoostClassifier(),
    XGBClassifier(),
    LGBMClassifier(verbose=-1),
    LinearSVC(),
    NuSVC(),
    SVC(),
    GaussianNB()
]

"""## Boucle avec print résultat"""

from sklearn.model_selection import cross_val_score

#double boucle
results = []
for model in list_model:

    for scaler in list_scaler:

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        model.fit(X_train_scaled, y_train)
        score = cross_val_score(model,X_train_scaled, y_train, scoring = 'recall', cv = 4)
        print(model.__class__.__name__, score)
        accuracy_train = model.score(X_train_scaled, y_train)
        accuracy_test = model.score(X_test_scaled, y_test)
        y_pred_test = model.predict(X_test_scaled)
        recall_test = recall_score(y_test, y_pred_test)
        overfitting = (model.score(X_train_scaled, y_train) - model.score(X_test_scaled, y_test))
        confusion_mat = confusion_matrix(y_test, y_pred_test)
        matrice = pd.DataFrame(data = confusion_mat,
                 index = model.classes_ ,
                 columns = model.classes_)
        results.append((model.__class__.__name__, scaler.__class__.__name__, accuracy_train, accuracy_test, recall_test, overfitting, matrice))

#tris de la liste results en fonction du recall_test, puis de l'accuracy test
sorted_results = sorted(results, key=lambda x: (x[4], x[3]), reverse=True)

for model, scaler, accuracy_train, accuracy_test, recall_test, overfitting, matrice in sorted_results:
    print("___________________________")
    print(f"Model: {model}")
    print(f"Scaler: {scaler}")
    print(f"Accuracy Train: {accuracy_train}")
    print(f"Accuracy Test: {accuracy_test}")
    print(f"Recall Test: {recall_test}")
    print(f"Overfitting: {overfitting}")
    print(matrice)

"""Model: ExtraTreesClassifier
Scaler: MaxAbsScaler
Accuracy Train: 1.0
Accuracy Test: 0.989247311827957
Recall Test: 0.9827586206896551
Overfitting: 0.010752688172043001

"""



"""# cross Validation sur le ExtraTreeClassifier"""

scaler = MaxAbsScaler()
X_train_scal= scaler.fit_transform(X_train)
X_test_scal= scaler.transform(X_test)

from sklearn.model_selection import cross_val_score
cross_val_score(ExtraTreesClassifier(), X_train_scal , y_train, scoring= 'recall')

df.info()

"""# randomizedsearch sur ExtraTreeClassifier

DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=None, splitter='best')
"""

from sklearn.model_selection import RandomizedSearchCV
dico = {'max_depth' : range(1,51), 'min_samples_leaf' : range(1,16), 'min_samples_split' : [2,5,7,10,15,30]}
rando = RandomizedSearchCV(ExtraTreesClassifier(), dico ,n_iter= 100, cv=5)
rando.fit(X_train_scal,y_train)

print("best score:",rando.best_score_)
print("best parameters:",rando.best_params_)
print("best estimator:",rando.best_estimator_)

"""## Ne pas prendre les best parametres pour ce DF sinon les scores seront tous à 1"""

modelETC= ExtraTreesClassifier()
modelETC.fit(X_train_scal, y_train)

# résultat avec best scaler, best model et best paramètre :
print(' accuracy score Train: ', modelETC.score(X_train_scal, y_train))
print(' accuracy score Test: ', modelETC.score(X_test_scal, y_test))

y_pred_test = modelETC.predict(X_test_scal)

recall_test = recall_score(y_test, y_pred_test)
recall_test

confusion_mat = confusion_matrix(y_test, y_pred_test)
matrice = pd.DataFrame(data = confusion_mat,
                 index = modelETC.classes_ ,
                 columns = modelETC.classes_)
matrice

"""## TEST mypredict"""

import numpy as np

a = 15.8
b= 1.1
c= 141
d= 53
e= 6.1
f= 18
g= 131
h= 0
i= 0
j= 0
k= 0
l= 58
m= 80

my_data = np.array([a,b,c,d,e,f,g,h,i,j,k,l,m]).reshape(1,13)

modelRein= ExtraTreesClassifier()
scal_rein= MaxAbsScaler()
X = df[['hemo','creatinine','sod','packed_cell_vol','red_blood_cell','blood_urea','blood_g_random',
                     'sugar','albumine', 'hypertension','diabete','age','blood_pressure']]
y = df['classification']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size = 0.75)
X_train_scal= scal_rein.fit_transform(X_train)
X_test_scal= scal_rein.transform(X_test)
modelRein.fit(X_train_scal, y_train)
rein_data = pd.DataFrame(my_data,
                    columns= ['hemo','creatinine','sod','packed_cell_vol','red_blood_cell','blood_urea','blood_g_random',
                     'sugar','albumine', 'hypertension','diabete','age','blood_pressure'])
rein_data_scal = scal_rein.transform(rein_data)
rein_predict = modelRein.predict(rein_data_scal)

int(rein_predict)

"""## test mypredict diabete"""

df_diabete= pd.read_csv(r'/content/df_diabete.csv')

a= 50
b= 6
c= 33.6
d= 206.846154
e= 148
f= 35
g= 72
h= 0.627

modelDiab = RandomForestClassifier(criterion = 'entropy', max_features = None, min_samples_leaf = 1, n_estimators = 15)
scal_diab= QuantileTransformer()
X = df_diabete[['Age','Pregnancies','BMI','Insulin', 'Glucose'
                     'SkinThickness','BloodPressure','DiabetesPedigreeFunction']]
y = df_diabete['Outcome']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size = 0.75)
X_train_scal= scal_diab.fit_transform(X_train)
X_test_scal= scal_diab.transform(X_test)
modelDiab.fit(X_train_scal, y_train)
diab_data = np.array([a,b,c,d,e,f,g,h]).reshape(1,8)
diab_data = pd.DataFrame(diab_data, columns= ['Age','Pregnancies','BMI','Insulin', 'Glucose'
                     'SkinThickness','BloodPressure','DiabetesPedigreeFunction'])
diab_data_scal = scal_diab.transform(diab_data)
diab_predict = modelDiab.predict(diab_data_scal)
print(diab_predict)

"""# SHAP VALUE"""

pip install shap

import shap

"""code de Lo"""

# Convertir le résultat en DataFrame avec les noms de colonnes
X_test_shape = pd.DataFrame(X_test_scal, columns=list(X_test.columns))
X_train_shape = pd.DataFrame(X_train_scal, columns=list(X_train.columns))

explainer = shap.KernelExplainer(modelETC.predict,shap.sample(X_test_shape,100))

shap_values = explainer.shap_values(X_test_shape,nsamples=100)

shap.summary_plot(shap_values,X_test_shape)

"""'hemo','packed_cell_vol','red_blood_cell', 'blood_g_random','sod','albumine','sugar','blood_urea', 'hypertension','age','blood_pressure','diabete'"""

shap.initjs()
num_test = 2
fig = plt.figure(figsize=(20,10))
shap.force_plot(explainer.expected_value, shap_values[num_test,:], X_test_scal[num_test,:].round(3), feature_names=X.columns, matplotlib=True, show=True)
plt.savefig('force_plot.png')

"""Chaque barre horizontale dans le graphique représente la contribution de chaque caractéristique à la différence entre la valeur prédite et la valeur attendue pour cet exemple. Le graphique vous permet de voir visuellement comment chaque caractéristique influence la prédiction du modèle pour cet exemple particulier."""



"""# LASSO CV"""

from sklearn.linear_model import LassoCV

reg = LassoCV()
reg.fit(X_train_scal, y_train)

#print("Best alpha using built-in LassoCV: %f" % reg.alphas_)
print("Best score using built-in LassoCV: %f" %reg.score(X_train_scal,y_train))
coef = pd.Series(reg.coef_, index = X_train.columns)
print("Lasso picked " + str(sum(coef != 0)) + " variables and eliminated the other " +  str(sum(coef == 0)) + " variables")

fig = plt.figure(figsize=(6,6))
imp_coef = coef.sort_values()
plt.rcParams['figure.figsize'] = (8.0, 10.0)
imp_coef.plot(kind = "barh")
plt.axvline(x=0, color='g')
plt.title("Importance des variables en utilisation un modèle Lasso")
fig.savefig('importance_graph.png', dpi=200, bbox_inches = 'tight') ;

df

modelRein= ExtraTreesClassifier()
scal_rein= MaxAbsScaler()
X = df[['blood_pressure', 'sugar', 'age', 'hypertension', 'diabete', 'pus_cell', 'appet',
        'coronary_artery_disease',
        'anemie', 'packed_cell_vol']]
y = df['classification']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size = 0.75)
X_train_scal= scal_rein.fit_transform(X_train)
X_test_scal= scal_rein.transform(X_test)
modelRein.fit(X_train_scal, y_train)

y_pred_test = modelRein.predict(X_test_scal)

accuracy_score(y_pred_test, y_test)

recall_score(y_pred_test, y_test)