# -*- coding: utf-8 -*-
"""Projet_3_Breast_cancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Foq-emK5_jlfGa542OSlJAErvulHA2w2

#1. Import et Exploration
"""

import pandas as pd
link = "https://raw.githubusercontent.com/MaskiVal/DataSets/main/cancer_breast.csv"
df = pd.read_csv(link)
df

df.info()

"""Aucun NaN, sauf pour la dernière colonne "Unnamed: 32" qui est entièrement vide"""

df = df.drop('Unnamed: 32', axis=1)

"""#2. Distributions des données & Corrélations

## Distribution des diagnostiques

Nombre de cellules diagnostiquées comme malignes ou bénines
"""

diag_count = df['diagnosis'].value_counts().reset_index()
diag_count

diag_count.columns = ['Diagnosis', 'Count']
diag_count

import plotly.express as px
fig = px.bar(diag_count, x='Diagnosis', y='Count',
             title='Répartition des diagnostics de cancer du sein')
fig.show()

fig = px.pie(diag_count, values='Count', names='Diagnosis',
             title='Répartition des diagnostics de cancer du sein')
fig.update_traces(textposition='inside', textinfo='percent+label')
fig.show()

"""==> Problème de répartition des 2 groupes de cellules.

A FAIRE : ML pour augmenter la proportion de cellules M par rapport aux cellules B

--> "diagnosis" est la seule variable catégorique/variable qualitative

## Variables numériques
"""

pd.set_option('display.max_columns', None)

var = df.drop(['id','diagnosis'],axis = 1 )
var

"""On a 30 variables numériques, sans compter l'id des cellules"""

var.describe()

"""Il y a des variables qui contiennent des zéros comme valeurs minimales, on va les afficher

## Gestion des zéros
"""

# df.eq(0) qui compare le dataframe avec la valeur zéro et renvoie True ou False, ajouter à any() cela renvoie les colonnes True
df.columns[df.eq(0).any()]

"""Toutes les variables liées à la concavité des cellules sont concernées par les zéros"""

df['concavity_mean'].value_counts().sort_index(ascending=True)

df['concave points_mean'].value_counts().sort_index(ascending=True)

df['concavity_se'].value_counts().sort_index(ascending=True)

df['concave points_se'].value_counts().sort_index(ascending=True)

df['concavity_worst'].value_counts().sort_index(ascending=True)

df['concave points_worst'].value_counts().sort_index(ascending=True)

df[df['concavity_mean']==0]

"""Cela concerne uniquement des cellules Bénines

Le client est d'accord pour supprimer les cellules ayant des zéros pour les variables concernant la concavité.

Mais n'est-il pas possible que la valeur réelle soit à = 0 ?

De plus peut-on faire un fill zéros avec la moyenne ou la médiane ?

## df.drop = df en retirant les 13 lignes contenant des zéros
"""

#df si on décide de juste supprimer, selon la consigne du client



df_drop = df[df['concavity_mean'] != 0]

df_drop.describe()

"""## df.fillzeros = df en remplaçant les zéros par mean/median"""

from scipy.stats import skew

colonnes_concavity = ['concavity_mean', 'concave points_mean', 'concavity_se',
                     'concave points_se', 'concavity_worst', 'concave points_worst']


def calculer_skewness_sans_zeros(df, colonnes):
    skewness_resultats = {}

    for col in colonnes:
        # Retirer les valeurs nulles
        non_null_values = df[col][df[col] != 0]

        # Calculer la skewness des valeurs non nulles
        skewness_value = non_null_values.skew()

        # Afficher le résultat
        print(f"Skewness de {col} sans les zéros : {skewness_value}")


calculer_skewness_sans_zeros(df, colonnes_concavity)

"""Positif = remplacer par median

proche de 0 = remplacer par mean ou median

Négatif = remplacer par mean
"""

# on remplace donc les zéros par la medianne de la colonne

df_fillzeros = df.copy()

# Remplissage des valeurs zéros avec la moyenne de chaque variable
for col in colonnes_concavity:
    col_mean = df[col].median()
    df_fillzeros[col] = df_fillzeros[col].replace(0, col_mean)


df_fillzeros.describe()

"""## Analyses générales

### Installation Pandas-Profiling
"""

import sys
import warnings
warnings.filterwarnings('ignore')

!pip install ydata-profiling

! pip install [https://github.com/pandas-profiling/pandas-profiling/archive/master.zip](https://github.com/pandas-profiling/pandas-profiling/archive/master.zip)

""" ### Pandas profiling report"""

#from pandas_profiling import ProfileReport

import numpy as np

#profile = ProfileReport(df, title="analyse_variables", html={'style' : {'full_width':True}})

#profile.to_notebook_iframe()

#profile.to_file(output_file = "Analyse_variables2.html")

"""### Graphiques

#### Variables mean :

On a besoin de changer d'avoir la colonne 'diagnostis' en format numérique
"""

df['diagnosis_num'] = df['diagnosis'].replace('B', 0).replace('M', 1)

df['diagnosis_num'] = df['diagnosis_num'].astype('int')

df.info()

var_mean = ['diagnosis_num','radius_mean','texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']
var_mean = df[var_mean].copy()
var_mean

import seaborn as sns

#sns.pairplot(car_predict_diag, hue = 'diagnosis_num', diag_kind="hist")

"""## Recherche de correlations

### Observation des 30 variables
"""

import seaborn as sns
import matplotlib.pyplot as plt

correlation_matrix = df.corr()

plt.figure(figsize=(20, 12))
sns.heatmap(correlation_matrix, linewidths = 1, annot = True, fmt = ".2f")
plt.title('Matrice de corrélation des 30 variables')
plt.show()

"""Il y a de nombreuses colonnes qui sont très fortement corrélées =  multicolinéarité

==> Est-ce possible de supprimer les variables hautement corrélées ?
"""

# Tris de la matrice de corrélation par valeur absolue (pour inclure les corrélations négatives)
sorted_corr = correlation_matrix.abs().unstack().sort_values(ascending=False)

# Sélectionner les paires de colonnes avec les corrélations les plus élevées
top_corr_pairs = sorted_corr[sorted_corr < 1].drop_duplicates().head(150)

# Afficher les corrélations les plus fortes
print("Corrélations les plus fortes :")
print(top_corr_pairs)

corr_df = pd.DataFrame({
    'Variable_1': [pair[0] for pair in top_corr_pairs.index],
    'Variable_2': [pair[1] for pair in top_corr_pairs.index],
    'Correlation': top_corr_pairs.values
})

corr_df

forte_corr = corr_df[corr_df['Correlation'] > 0.90]
forte_corr

forte_corr = corr_df[corr_df['Correlation'] > 0.90]
forte_corr

forte_corr = forte_corr.sort_values(by = 'Variable_1')
forte_corr

"""### Observation 10 variables mean"""

#correlation
correlation_matrix2 = var_mean.corr()

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix2, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matrice de corrélation des variables mean ')
plt.show()

sorted_corr2 = correlation_matrix2.abs().unstack().sort_values(ascending=False)

top_corr_pairs2 = sorted_corr2[sorted_corr2 < 1].drop_duplicates().head(150)

print("Corrélations les plus fortes :")
print(top_corr_pairs2)

corr_df2 = pd.DataFrame({
    'Variable_1': [pair[0] for pair in top_corr_pairs2.index],
    'Variable_2': [pair[1] for pair in top_corr_pairs2.index],
    'Correlation': top_corr_pairs2.values
})

corr_df2

forte_corr2 = corr_df2[corr_df2['Correlation'] > 0.80]
forte_corr2

forte_corr2 = forte_corr2.sort_values(by = 'Variable_1')
forte_corr2

"""### Distribution des variables en fonction du diagnostique :"""

import plotly.express as px

variables = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']

for variable in variables:
    # Box plot
    fig = px.box(var_mean, x='diagnosis_num', y=variable,
                 title=f'Distribution de {variable} en fonction du diagnostic',
                 labels={'diagnosis': 'Diagnosis', variable: variable})
    fig.show()

    # Histogram
    fig = px.histogram(var_mean, x=variable, color='diagnosis_num', title=f'Distribution de {variable} en fonction du diagnostic')
    fig.show()

"""#3. Statistiques

### import
"""

pip install statannot

from statannot import add_stat_annotation
from scipy import stats

"""### p-value"""

#df_drop

df2 = df_drop.copy()

df2.info()

columns_of_interest = [
    'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',
    'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean',
    'symmetry_mean', 'fractal_dimension_mean'
]

fig, axes = plt.subplots(6, 6, figsize=(18, 18))
plt.subplots_adjust(hspace=0.5)

def add_stat_annotations(x, y, ax):
    add_stat_annotation(data=df, x=x, y=y, ax=ax, box_pairs=[('M', 'B')],
                        perform_stat_test=True, test='Kruskal', text_format='star', loc='outside', verbose=2)

for i, column in enumerate(columns_of_interest):
    row, col = divmod(i, 6)
    sns.boxplot(x=df['diagnosis'], y=df[column], ax=axes[row, col])
    axes[row, col].set_title(column)
    add_stat_annotations('diagnosis', column, axes[row, col])

plt.show()

columns_of_interest = [
    'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',
    'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean',
    'symmetry_mean', 'fractal_dimension_mean'
]

# Calcule le nombre de lignes et de colonnes nécessaires en fonction de la longueur de columns_of_interest
num_columns = len(columns_of_interest)
num_rows = (num_columns + 2) // 3

fig, axes = plt.subplots(num_rows, 3, figsize=(18, 3 * num_rows))
plt.subplots_adjust(hspace=0.5)

def add_stat_annotations(x, y, ax):
    add_stat_annotation(data=df, x=x, y=y, ax=ax, box_pairs=[('M', 'B')],
                        perform_stat_test=True, test='Kruskal', text_format='star', loc='outside', verbose=2)

for i, column in enumerate(columns_of_interest):
    row, col = divmod(i, 3)
    sns.boxplot(x=df['diagnosis'], y=df[column], ax=axes[row, col])
    axes[row, col].set_title(column)
    add_stat_annotations('diagnosis', column, axes[row, col])

plt.show()

"""## Df à utiliser pour les algorithmes des machines learning"""

df_drop.info()

#variables à sélectionner
#variables mean
var3 = ['diagnosis','radius_mean','texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']

#df final
df_clean = df_drop[var3]
df_clean

test = df_clean[df_clean['diagnosis'] == 'B']
test

"""Df pour l'application DASH"""

df_clean_all_var = df_drop.copy()

df_clean_all_var['diagnosis'] = df_clean_all_var['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)

df_clean_all_var['diagnosis'].value_counts()

"""Df réquilibré"""

df_clean_all_var['diagnosis'].value_counts()

from imblearn.over_sampling import RandomOverSampler
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

X = df_clean_all_var.drop(['diagnosis'], axis=1)
y = df_clean_all_var['diagnosis']

# Diviser le jeu de données en ensembles d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Initialiser le sur-échantillonneur
ros = RandomOverSampler(random_state=42)

# Appliquer le sur-échantillonnage
X_resampled, y_resampled = ros.fit_resample(X, y)

# Nouveau DataFrame avec les données sur-échantillonnées
df_clean_all_var_reajuste = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name='diagnosis')], axis=1)

df_clean_all_var_reajuste['diagnosis'].value_counts()

df_clean_all_var_reajuste.to_csv('dataset_sein_requilibre.csv', index=False)

"""#ETAPES - Preprocessing - ML pour le df avec les variables mean

## PyCaret
"""

#faire un pycaret et observer les ML qui ressortent en premier

#pip install pycaret

#from pycaret.classification import *

#exp = setup(df_ml, target='diagnosis')

#best_model = compare_models()

"""## IMPORT"""

pip install lightgbm

from sklearn.model_selection import train_test_split
#pour les différents Scaler
from sklearn.preprocessing import MaxAbsScaler, QuantileTransformer, PowerTransformer, MinMaxScaler, StandardScaler, RobustScaler
#pour les différents modèles de ML
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier
from sklearn.svm import LinearSVC, NuSVC, SVC
from sklearn.naive_bayes import GaussianNB
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier

#pour les metrics
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import recall_score

from sklearn.model_selection import cross_val_score

"""## Automatisation des paramètres du df_final

#####1 - Défini quel df utiliser

utilise un df qui contient uniquement les variables que tu souhaite utiliser

df_final = ton df.copy()
"""

df_final = df_clean_all_var_reajuste.copy()
df_final.head()

"""#####2 - Définis ta variable TARGET

var_target = 'nom_de_ta_variable'
"""

target_variable = 'diagnosis'

df_final[target_variable].unique()

"""##### 3 - (optionel )Numérise ta variable TARGET  si ce n'est pas déjà le cas : malade = 1; non malade = 0

positive_class_value = 'nom_valeur_malade"
"""

#positive_class_value = 'M'

#df_final[target_variable] = df_final[target_variable].apply(lambda x: 1 if x == positive_class_value else 0)

#df_final[target_variable].value_counts()

"""## Initialisation du ML"""

#définir y et X
var = ['radius_mean','texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']
X = df_final[var]
y = df_final[target_variable]

X

#train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("The length of the initial dataset is :", len(X))
print("The length of the train dataset is   :", len(X_train))
print("The length of the test dataset is    :", len(X_test))

"""## TOP modèles ML avec son best Scaler et cross validation"""

#listes

##liste des Scalers utilisés
list_scalers = [
    MaxAbsScaler(),
    QuantileTransformer(),
    PowerTransformer(),
    MinMaxScaler(),
    StandardScaler(),
    RobustScaler()
]
##liste des modèles utilisés
list_models = [
    KNeighborsClassifier(),
    DecisionTreeClassifier(),
    LogisticRegression(),
    ExtraTreesClassifier(),
    RandomForestClassifier(),
    GradientBoostingClassifier(),
    AdaBoostClassifier(),
    XGBClassifier(),
    LGBMClassifier(verbose=-1),
    LinearSVC(),
    NuSVC(),
    SVC(),
    GaussianNB()
]

#double boucle
results = []
for model in list_models:

    for scaler in list_scalers:

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        model.fit(X_train_scaled, y_train)

        accuracy_train = model.score(X_train_scaled, y_train)
        accuracy_test = model.score(X_test_scaled, y_test)

        y_pred_test = model.predict(X_test_scaled)

        recall_test = recall_score(y_test, y_pred_test)
        overfitting = (model.score(X_train_scaled, y_train) - model.score(X_test_scaled, y_test))

        scores = cross_val_score(model, X_train_scaled, y_train, scoring='recall', cv=6)
        avg_scores = scores.mean()

        confusion_mat = confusion_matrix(y_test, y_pred_test)
        matrice = pd.DataFrame(data = confusion_mat,
                 index = model.classes_ ,
                 columns = model.classes_)

        results.append((model.__class__.__name__, scaler.__class__.__name__, accuracy_train, accuracy_test, recall_test, overfitting, avg_scores, matrice))

#tris de la liste results en fonction de la moyenne des scores de cross_validation, puis du recall_test, puis de l'accuracy test
sorted_results = sorted(results, key=lambda x: (x[6], x[4], x[3]), reverse=True)

for model, scaler, accuracy_train, accuracy_test, recall_test, overfitting, avg_scores, matrice in sorted_results:
    print("___________________________")
    print(f"Model: {model}")
    print(f"Scaler: {scaler}")
    print(f"Avg scores - cross validation : {avg_scores}")
    print(f"Accuracy Train: {accuracy_train}")
    print(f"Accuracy Test: {accuracy_test}")
    print(f"Recall Test: {recall_test}")
    print(f"Overfitting: {overfitting}")
    print(matrice)

"""On veut sélectionner le couple Modèle de ML + Sacler le plus performant tout en évitant les metrics où l'accuracy train est égale à 1

## Cross Validation
"""

#Cross-validation données de test
model = KNeighborsClassifier()
scaler = QuantileTransformer()
X_train_scaled = scaler.fit_transform(X_train)
model.fit(X_train_scaled, y_train)
X_test_scaled = scaler.transform(X_test)
y_pred_test = model.predict(X_test_scaled)

scores = cross_val_score(model, X_train_scaled, y_train, cv=6, scoring='recall')

scores

"""Plus il y a d'écart entre les valeurs, moins nous avons confiance dans la métrique (ca signifie que le découpage a beaucoup d'influence sur notre score). Ici les valeurs sont relativement proches, ce qui me donne confiance en mon modèle de prédiction"""

# moyenne des métriques
scores.mean()

# ecart-type des métriques
scores.std()

"""L'écart-type est faible = cohérence dans les performances du modèle"""

accuracy_test = model.score(X_test_scaled, y_test)
recall_test = recall_score(y_test, y_pred_test)

print("Metrics for KNN Model before best parameters:")
print(f"Accuracy Test: {accuracy_test:.4f}")
print(f"Recall Test: {recall_test:.4f}")

# données d'entrainement

y_pred_train = model.predict(X_train_scaled)

accuracy_train = accuracy_score(y_train, y_pred_train)
recall_train = recall_score(y_train, y_pred_train)

print(f"Accuracy Train: {accuracy_train:.4f}")
print(f"Recall Train: {recall_train:.4f}")

"""## GridSearch

"""

from sklearn.model_selection import GridSearchCV
parameters = {'n_neighbors' : range(1, 101),
              'weights' : ['uniform', 'distance']}

gsearch = GridSearchCV(model, parameters, cv = 6)
gsearch.fit(X_train_scaled, y_train)

#meilleures valeurs des hyperparamètres
print(f"Best accuracy score: {gsearch.best_score_:.5f}")
print(f"Best parameters: {gsearch.best_params_}")
print(f"Best estimator: {gsearch.best_estimator_}")

"""## Model et Scaler avec best parameters"""

#gridsearch
best_model_g = KNeighborsClassifier(n_neighbors = 5, weights = 'uniform')

X_train_scaled_g = scaler.fit_transform(X_train)
best_model_g.fit(X_train_scaled_g, y_train)
X_test_scaled_g = scaler.transform(X_test)

y_pred_g = best_model_g.predict(X_test_scaled_g)
accuracy_g = accuracy_score(y_test, y_pred_g)
recall_g = recall_score(y_test, y_pred_g)
confusion_mat_g = confusion_matrix(y_test, y_pred_g)

print("Metrics for GridSearch Model:")
print(f"Accuracy: {accuracy_g:.4f}")
print(f"Recall: {recall_g:.4f}")

# matrice de confusion
confusion_mat = confusion_matrix(y_test, y_pred_g)
classes = ['patients sains', 'patients malades']


matrice = pd.DataFrame(data=confusion_mat,
                       index=[f'{cls} REELS' for cls in classes],
                       columns=[f'{cls} PREDITS' for cls in classes])

# heatmap
plt.figure(figsize=(6, 6))
sns.heatmap(matrice, annot=True, cmap='Blues', fmt='g')


plt.xlabel('Prédiction')
plt.ylabel('Valeur réelle')
plt.title('Matrice de Confusion')

plt.show()

"""## PREDICTIONS"""

patient_malade_index_0 = [[17.99, 10.38, 122.80, 1001.0, 0.11840, 0.27760, 0.30010, 0.14710, 0.2419, 0.07871]]

patient_malade_index_0_scal = scaler.transform(patient_malade_index_0)

pred = model.predict(patient_malade_index_0_scal)

print(f"Prediction: {pred}")

proba_pred = model.predict_proba(patient_malade_index_0_scal)
proba_pred

patient_non_malade_index_19 = [[13.54, 14.36, 87.46, 566.3, 0.09779, 0.08129, 0.06664, 0.04781, 0.1885, 0.05766]]

patient_non_malade_index_19_scal = scaler.transform(patient_non_malade_index_19)

pred2 = model.predict(patient_non_malade_index_19_scal)

print(f"Prediction: {pred2}")

proba_pred2 = model.predict_proba(patient_non_malade_index_19_scal)
proba_pred2

# Obtenir les classes prédites par le modèle
classes = model.classes_

# Afficher les classes
print("Classes prédites :", classes)

# Afficher les probabilités de prédiction
print("Probabilités de prédiction :", proba_pred2)

nouveau_patient = [[15.0, 20.0, 100.0, 800.0, 0.1, 0.2, 0.25, 0.15, 0.3, 0.05]]

nouveau_patient_scal = scaler.transform(nouveau_patient)

pred3 = model.predict(nouveau_patient_scal)

print(f"Prediction: {pred3}")

proba_pred3 = model.predict_proba(nouveau_patient_scal)
proba_pred3

"""## Feature_importance"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.inspection import permutation_importance

result = permutation_importance(best_model_g, X_train_scaled_g, y_train, n_repeats=10, random_state=42)

feature_importance_svc = pd.DataFrame({'Feature': X.columns, 'Importance': result.importances_mean})
feature_importance_svc = feature_importance_svc.sort_values('Importance', ascending=True)

feature_importance_svc.plot(x='Feature', y='Importance', kind='barh', figsize=(10, 10))
plt.title('Feature Importance for SVC with MinMaxScaler')
plt.show()

"""## LASSO CV"""

from sklearn.linear_model import LassoCV

reg = LassoCV()
reg.fit(X_train_scaled_g, y_train)

#print("Best alpha using built-in LassoCV: %f" % reg.alphas_)
print("Best score using built-in LassoCV: %f" %reg.score(X_train_scaled_g,y_train))
coef = pd.Series(reg.coef_, index = X_train.columns)
print("Lasso picked " + str(sum(coef != 0)) + " variables and eliminated the other " +  str(sum(coef == 0)) + " variables")

fig = plt.figure(figsize=(6,6))
imp_coef = coef.sort_values()
plt.rcParams['figure.figsize'] = (8.0, 10.0)
imp_coef.plot(kind = "barh")
plt.axvline(x=0, color='g')
plt.title("Importance des variables en utilisation un modèle Lasso")
fig.savefig('importance_graph.png', dpi=200, bbox_inches = 'tight') ;

"""On peut donc retirer les variables fractal_dimensions, perimeter et radius qui n'apportent rien dans nos algorithmes

## SHAP
"""

pip install shap

import shap

# Convertir le résultat en DataFrame avec les noms de colonnes
X_test_shape = pd.DataFrame(X_test_scaled_g, columns=list(X_test.columns))
X_train_shape = pd.DataFrame(X_train_scaled_g, columns=list(X_train.columns))

explainer = shap.KernelExplainer(best_model_g.predict,shap.sample(X_test_shape,100))

shap_values = explainer.shap_values(X_test_shape,nsamples=100)

shap.summary_plot(shap_values,X_test_shape)

"""# DEEP LEARNING RESEAU DE NEURONNE"""

# Pour le DEEP LEARNING
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.neural_network import MLPClassifier

QTscaler = QuantileTransformer()

X = df_final.drop(target_variable, axis=1)
y = df_final[target_variable]

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size = 0.75)

X_train_scaled = QTscaler.fit_transform(X_train)
X_test_scaled = QTscaler.transform(X_test)

print(X_train_scaled.shape)

"""## Keras"""

modelK = Sequential()

#modelMLP.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))
#modelMLP.add(Dense(32, activation='relu'))
#modelMLP.add(Dense(1, activation='sigmoid'))

modelK.add(Dense(64,activation='relu'))
modelK.add(Dropout(0.2))

modelK.add(Dense(32,activation='relu'))
modelK.add(Dropout(0.2))

modelK.add(Dense(16,activation='relu'))

modelK.add(Dense(1,activation='sigmoid'))

modelK.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy','Recall'])
#modelMLP.compile(optimizer=Adam(), loss='binary_crossentropy')

#early_stop=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=9)

modelK.fit(X_train_scaled, y_train, epochs=40, validation_data=[X_test_scaled, y_test], validation_split=0.2)

loss, accuracy ,recall = modelK.evaluate(X_test_scaled, y_test)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")
print(f"Test Recall: {recall:.4f}")

"""## MLP via skleanr + optim hyperparametres"""

param_grid = {
    'hidden_layer_sizes': [(64,), (128,), (256,)],
    'activation': ['relu', 'tanh', 'sigmoid'],
    'learning_rate_init': [0.001, 0.01, 0.1]
}

mlp_model = MLPClassifier(max_iter=1000)

def custom_scorer(y_true, y_pred):
    recall = recall_score(y_true, y_pred)
    accuracy = accuracy_score(y_true, y_pred)
    # Poids arbitraire pour favoriser le recall
    return 0.8 * recall + 0.2 * accuracy

grid_search = GridSearchCV(mlp_model, param_grid, cv=3, scoring='recall', verbose=1)
grid_search.fit(X_train_scaled, y_train)

print("Meilleurs hyperparamètres:", grid_search.best_params_)

final_model = grid_search.best_estimator_

y_pred = final_model.predict(X_test_scaled)

final_recall = recall_score(y_test, y_pred)
final_accuracy = accuracy_score(y_test, y_pred)

print("Accuracy sur l'ensemble de test:", final_accuracy)
print("Recall sur l'ensemble de test:", final_recall)